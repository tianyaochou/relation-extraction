{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading BERT...\n"
     ]
    }
   ],
   "source": [
    "from bert import *\n",
    "\n",
    "print(\"Loading BERT...\")\n",
    "trained = False\n",
    "try:\n",
    "    model = BertForSequenceClassification.from_pretrained(\"./results\")\n",
    "    trained = False\n",
    "except:\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=10\n",
    "    )\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'Other': 929, 'Entity-Destination': 929, 'Component-Whole': 929, 'Member-Collection': 929, 'Instrument-Agency': 929, 'Entity-Origin': 929, 'Product-Producer': 929, 'Message-Topic': 929, 'Cause-Effect': 929, 'Content-Container': 929})\nCounter({'Other': 929, 'Entity-Destination': 929, 'Component-Whole': 929, 'Member-Collection': 929, 'Instrument-Agency': 929, 'Entity-Origin': 929, 'Product-Producer': 929, 'Message-Topic': 929, 'Cause-Effect': 929, 'Content-Container': 929})\n"
     ]
    }
   ],
   "source": [
    "# Split Dataset into 0.2 test and 0.8 train\n",
    "raw_data = Dataset(\"./train.txt\")\n",
    "\n",
    "# Oversampling data\n",
    "from collections import Counter\n",
    "count = Counter([label[0] for label in raw_data.label_train])\n",
    "print(count)\n",
    "\n",
    "# for idx in range(len(raw_data.label_train)):\n",
    "#     if raw_data.label_train[idx][0] in {\"Content-Container\",\"Instrument-Agency\"}:\n",
    "#         raw_data.data_train.append(raw_data.data_train[idx])\n",
    "#         raw_data.label_train.append(raw_data.label_train[idx])\n",
    "\n",
    "from collections import Counter\n",
    "count = Counter([label[0] for label in raw_data.label_train])\n",
    "print(count)\n",
    "\n",
    "# Preparing Dataset\n",
    "train_encodings = tokenizer(raw_data.data_train, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(raw_data.data_test, truncation=True, padding=True)\n",
    "label_encoder = LabelEncoder()\n",
    "train_label_ids = label_encoder.fit_transform([label[0] for label in raw_data.label_train])\n",
    "train_dataset = RelationExtractionDataset(\n",
    "    train_encodings,\n",
    "    train_label_ids\n",
    ")\n",
    "test_label_ids = label_encoder.transform([label[0] for label in raw_data.label_test])\n",
    "test_dataset = RelationExtractionDataset(\n",
    "    test_encodings,\n",
    "    test_label_ids\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # output directory\n",
    "    num_train_epochs=3,  # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir=\"./logs\",  # directory for storing logs\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # the instantiated huggingface Transformers model to be trained\n",
    "    args=training_args,  # training arguments, defined above\n",
    "    train_dataset=train_dataset,  # training dataset\n",
    "    eval_dataset=test_dataset,  # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 14%|█▍        | 500/3486 [00:48<04:51, 10.25it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 104.05it/s]\u001b[A{'loss': 0.14686508178710939, 'learning_rate': 5e-05, 'epoch': 0.43029259896729777}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 100.11it/s]\u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 98.10it/s] \u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 97.02it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 96.00it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 95.58it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 95.28it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 95.07it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 94.66it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 94.37it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 94.17it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 94.30it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 94.12it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 94.26it/s]\u001b[A\n",
      "\n",
      " 14%|█▍        | 500/3486 [00:50<04:51, 10.25it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 94.36it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.4032077193260193, 'epoch': 0.43029259896729777}\n",
      " 29%|██▊       | 1000/3486 [01:40<04:00, 10.34it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 103.08it/s]\u001b[A{'loss': 0.18433027648925782, 'learning_rate': 4.1627595445411924e-05, 'epoch': 0.8605851979345955}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 99.48it/s] \u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 97.39it/s]\u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 95.98it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 95.02it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 94.35it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 94.42it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 94.21it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 94.06it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 93.69it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 93.17it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 93.33it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 93.44it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 93.79it/s]\u001b[A\n",
      "\n",
      " 29%|██▊       | 1000/3486 [01:42<04:00, 10.34it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 93.50it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.23626497387886047, 'epoch': 0.8605851979345955}\n",
      " 43%|████▎     | 1500/3486 [02:32<03:10, 10.42it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 104.05it/s]\u001b[A{'loss': 0.10571756744384765, 'learning_rate': 3.3255190890823845e-05, 'epoch': 1.2908777969018934}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 100.72it/s]\u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 97.93it/s] \u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 96.62it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 95.73it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 95.39it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 94.88it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 94.52it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 94.28it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 93.84it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 93.80it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 92.99it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 93.20it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 93.09it/s]\u001b[A\n",
      "\n",
      " 43%|████▎     | 1500/3486 [02:34<03:10, 10.42it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 93.28it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.2195364236831665, 'epoch': 1.2908777969018934}\n",
      " 57%|█████▋    | 2000/3486 [03:25<02:22, 10.41it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 103.08it/s]\u001b[A{'loss': 0.06738605499267578, 'learning_rate': 2.488278633623577e-05, 'epoch': 1.721170395869191}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 99.48it/s] \u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 97.68it/s]\u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 95.90it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 94.96it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 94.31it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 93.87it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 93.56it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 93.34it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 92.93it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 93.16it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 92.81it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 93.08it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 93.00it/s]\u001b[A\n",
      "\n",
      " 57%|█████▋    | 2000/3486 [03:26<02:22, 10.41it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 92.96it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.2277631014585495, 'epoch': 1.721170395869191}\n",
      " 72%|███████▏  | 2500/3486 [04:16<01:31, 10.83it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 105.04it/s]\u001b[A{'loss': 0.05194240570068359, 'learning_rate': 1.651038178164769e-05, 'epoch': 2.1514629948364887}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 101.36it/s]\u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 98.94it/s] \u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 97.88it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 96.59it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 95.98it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 95.56it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 95.27it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 94.79it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 94.73it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 94.69it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 94.66it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 94.37it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 94.17it/s]\u001b[A\n",
      "\n",
      " 72%|███████▏  | 2500/3486 [04:18<01:31, 10.83it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 94.03it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.22677560150623322, 'epoch': 2.1514629948364887}\n",
      " 86%|████████▌ | 3000/3486 [05:06<00:44, 10.87it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 11/160 [00:00<00:01, 104.05it/s]\u001b[A{'loss': 0.013662210464477539, 'learning_rate': 8.137977227059612e-06, 'epoch': 2.581755593803787}\n",
      "\n",
      " 13%|█▎        | 21/160 [00:00<00:01, 101.02it/s]\u001b[A\n",
      " 19%|█▉        | 31/160 [00:00<00:01, 98.71it/s] \u001b[A\n",
      " 26%|██▌       | 41/160 [00:00<00:01, 97.15it/s]\u001b[A\n",
      " 32%|███▏      | 51/160 [00:00<00:01, 96.09it/s]\u001b[A\n",
      " 38%|███▊      | 61/160 [00:00<00:01, 95.09it/s]\u001b[A\n",
      " 44%|████▍     | 71/160 [00:00<00:00, 94.94it/s]\u001b[A\n",
      " 51%|█████     | 81/160 [00:00<00:00, 94.57it/s]\u001b[A\n",
      " 57%|█████▋    | 91/160 [00:00<00:00, 94.31it/s]\u001b[A\n",
      " 63%|██████▎   | 101/160 [00:01<00:00, 94.39it/s]\u001b[A\n",
      " 69%|██████▉   | 111/160 [00:01<00:00, 94.45it/s]\u001b[A\n",
      " 76%|███████▌  | 121/160 [00:01<00:00, 93.96it/s]\u001b[A\n",
      " 82%|████████▏ | 131/160 [00:01<00:00, 94.15it/s]\u001b[A\n",
      " 88%|████████▊ | 141/160 [00:01<00:00, 94.02it/s]\u001b[A\n",
      "\n",
      " 86%|████████▌ | 3000/3486 [05:08<00:44, 10.87it/s]\n",
      "100%|██████████| 160/160 [00:01<00:00, 94.19it/s]\u001b[A\n",
      "                                                 \u001b[A{'eval_loss': 0.23235607147216797, 'epoch': 2.581755593803787}\n",
      "100%|██████████| 3486/3486 [05:57<00:00,  9.75it/s]{'epoch': 3.0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not trained:\n",
    "    trainer.train()\n",
    "else:\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 8/160 [00:00<00:01, 79.10it/s]\u001b[A\n",
      " 10%|█         | 16/160 [00:00<00:01, 77.14it/s]\u001b[A\n",
      " 15%|█▌        | 24/160 [00:00<00:01, 77.05it/s]\u001b[A\n",
      " 20%|██        | 32/160 [00:00<00:01, 76.99it/s]\u001b[A\n",
      " 25%|██▌       | 40/160 [00:00<00:01, 76.98it/s]\u001b[A\n",
      " 30%|███       | 48/160 [00:00<00:01, 76.97it/s]\u001b[A\n",
      " 35%|███▌      | 56/160 [00:00<00:01, 76.96it/s]\u001b[A\n",
      " 40%|████      | 64/160 [00:00<00:01, 77.08it/s]\u001b[A\n",
      " 45%|████▌     | 72/160 [00:00<00:01, 77.11it/s]\u001b[A\n",
      " 50%|█████     | 80/160 [00:01<00:01, 77.06it/s]\u001b[A\n",
      " 55%|█████▌    | 88/160 [00:01<00:00, 76.81it/s]\u001b[A\n",
      " 60%|██████    | 96/160 [00:01<00:00, 77.04it/s]\u001b[A\n",
      " 65%|██████▌   | 104/160 [00:01<00:00, 77.22it/s]\u001b[A\n",
      " 70%|███████   | 112/160 [00:01<00:00, 77.37it/s]\u001b[A\n",
      " 75%|███████▌  | 120/160 [00:01<00:00, 77.24it/s]\u001b[A\n",
      " 80%|████████  | 128/160 [00:01<00:00, 77.58it/s]\u001b[A\n",
      " 85%|████████▌ | 136/160 [00:01<00:00, 76.85it/s]\u001b[A\n",
      " 90%|█████████ | 144/160 [00:01<00:00, 76.74it/s]\u001b[A\n",
      " 95%|█████████▌| 152/160 [00:01<00:00, 77.01it/s]\u001b[A\n",
      "100%|██████████| 160/160 [00:02<00:00, 76.52it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5048869848251343, 'epoch': 8.0}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "source": [
    "Output result on test.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 94%|█████████▍| 151/160 [00:01<00:00, 94.14it/s]\n",
      "Correct: 620 of 640, 0.96875\n"
     ]
    }
   ],
   "source": [
    "# Acurracy\n",
    "\n",
    "raw_score = trainer.predict(test_dataset, [\"labels\"])[0]\n",
    "score = torch.softmax(torch.tensor(raw_score), 1, torch.float32)\n",
    "label_ids = [line.argmax() for line in score]\n",
    "correct_num = 0\n",
    "for idx, result in enumerate(label_ids):\n",
    "    if result == test_label_ids[idx]:\n",
    "        correct_num += 1\n",
    "print(f\"\\nCorrect: {correct_num} of {len(test_label_ids)}, {correct_num / len(test_label_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Cause-Effect' 'Component-Whole' 'Content-Container' 'Entity-Destination'\n",
      " 'Entity-Origin' 'Instrument-Agency' 'Member-Collection' 'Message-Topic'\n",
      " 'Other' 'Product-Producer']\n",
      "0.97, 0.00, 0.00, 0.00, 0.02, 0.00, 0.00, 0.00, 0.02, 0.00, \n",
      "0.01, 0.96, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.01, 0.00, \n",
      "0.00, 0.00, 0.97, 0.00, 0.00, 0.00, 0.00, 0.00, 0.03, 0.00, \n",
      "0.00, 0.00, 0.00, 0.99, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, \n",
      "0.00, 0.00, 0.00, 0.00, 0.97, 0.00, 0.00, 0.00, 0.01, 0.01, \n",
      "0.00, 0.00, 0.00, 0.00, 0.00, 0.97, 0.00, 0.00, 0.03, 0.00, \n",
      "0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.98, 0.00, 0.02, 0.00, \n",
      "0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, \n",
      "0.01, 0.00, 0.00, 0.00, 0.00, 0.02, 0.02, 0.01, 0.93, 0.01, \n",
      "0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.00, 0.00, 0.00, 0.98, \n",
      "100%|██████████| 160/160 [00:12<00:00, 94.14it/s]"
     ]
    }
   ],
   "source": [
    "def print_matrix(real_label, predict_label):\n",
    "    ret = numpy.zeros((10,10), dtype=numpy.float)\n",
    "    for i in range(len(real_label)):\n",
    "        ret[real_label[i]][predict_label[i]] += 1\n",
    "    ret_sum = numpy.sum(ret, axis=1)\n",
    "    return numpy.matmul(numpy.diag(1 / ret_sum), ret)\n",
    "m = print_matrix(test_label_ids.tolist(), [t.item() for t in label_ids])\n",
    "print(label_encoder.inverse_transform(list(range(10))))\n",
    "for i in m:\n",
    "    for j in i:\n",
    "        print(\"%.2f\" % (j), end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "559it [00:21, 91.38it/s]"
     ]
    }
   ],
   "source": [
    "test_file = open(\"./test.txt\")\n",
    "text = []\n",
    "for line in test_file:\n",
    "    quote_index = line.index('\"')\n",
    "    text.append(line[quote_index + 1 : -2])\n",
    "\n",
    "text_batch = tokenizer(text, truncation=True, padding=True)\n",
    "test_batch = RelationExtractionDataset(text_batch, torch.zeros(len(text)))\n",
    "raw_score = trainer.predict(test_batch, [\"labels\"])[0]\n",
    "score = torch.softmax(torch.tensor(raw_score), 1, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = [line.argmax() for line in score]\n",
    "labels = label_encoder.inverse_transform(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"./output.txt\", \"w\")\n",
    "output_file.writelines([line + \"\\n\" for line in labels])\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({'Instrument-Agency': 2052, 'Content-Container': 2052, 'Component-Whole': 1026, 'Other': 1026, 'Member-Collection': 1026, 'Cause-Effect': 1026, 'Entity-Destination': 1026, 'Message-Topic': 1026, 'Product-Producer': 1026, 'Entity-Origin': 1026})\n"
     ]
    }
   ],
   "source": [
    "data_statistics = Counter([label[0] for label in raw_data.label_train])\n",
    "print(data_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}